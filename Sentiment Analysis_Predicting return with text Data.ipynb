{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import os\n",
    "import alphalens as al\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingDataPath=os.path.abspath('./Data/train')\n",
    "TrainingTickerList=os.listdir(TrainingDataPath)\n",
    "if '.DS_Store' in TrainingTickerList:\n",
    "    TrainingTickerList.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TrainingTickerList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = RegexpTokenizer(r'[^\\d\\W]+')\n",
    "sno = SnowballStemmer('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words |= {\"may\", \"business\", \"company\", \"could\", \"service\", \"result\", \"product\", \n",
    "               \"operation\", \"include\", \"law\", \"tax\", \"change\", \"financial\", \"require\",\n",
    "               \"cost\", \"market\", \"also\", \"user\", \"plan\", \"actual\", \"cash\", \"other\",\n",
    "               \"thereto\", \"thereof\", \"therefore\", \"bloomberg\",\"email\",\"photograph\",\n",
    "               \"bloombergquint\",\"productsbloomberg\",\"loginbloomberg\", \"bloombergconnect\",\n",
    "               \"customersbloomberg\",\"inclusioninnovationphilanthropysustainabilitybloomberg\",\"londonbloomberg\",\n",
    "               \"distributionbloomberg\",\"lawbloomberg\",\"taxbloomberg\",\"governmentbloomberg\",\"environmentbloombergnef\",\n",
    "               \"mediabloomberg\",\"marketsbloomberg\",\"technologybloomberg\",\"pursuitsbloomberg\",\"politicsbloomberg\",\n",
    "               \"opinionbloomberg\",\"businessweekbloomberg\",\"conferencesbloomberg\", \"appsbloomberg\",\n",
    "               \"radiobloomberg\",\"servicesbloomberg\",\"onbloomberg\",\"clipsbloomberg\",\"saysbloomberg\",\"bloombergolaf\",\n",
    "               \"bloombergwhat\",\"loginsoftwar\",'followfacebooktwitterlinkedininstagram',\n",
    "               'facebooktwitterinstagramlinkedin','twitterfacebook'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    #txt: text body string\n",
    "    #return: cleaned tockens list\n",
    "    lemm_txt = [ wnl.lemmatize(wnl.lemmatize(w.lower(),'n'),'v') \\\n",
    "                for w in word_tokenizer.tokenize(txt)[1:] if \\\n",
    "                w.isalpha() and \"bloomberg\" not in w and w not in stop_words \\\n",
    "                and wnl.lemmatize(wnl.lemmatize(w.lower(),'n'),'v') not in stop_words]\n",
    "    return [ sno.stem(w) for w in lemm_txt if w not in stop_words and len(w) > 2 and \"bloomberg\" not in w and sno.stem(w) not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(words):\n",
    "    # words: cleaned tokens list\n",
    "    dictWords = defaultdict(int)\n",
    "    for word in words:\n",
    "        dictWords[word] +=1\n",
    "    return dictWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def struct_article_data(datapath,tickerlist):\n",
    "    articles=[]\n",
    "    for ticker in tickerlist:\n",
    "        folder=os.path.join(datapath,ticker) # folder path\n",
    "        docname=os.listdir(folder)\n",
    "        if '.DS_Store' in docname:\n",
    "            docname.remove('.DS_Store')\n",
    "        \n",
    "        for doc in docname: # eg: doc='20190115_+0.036786_0.txt'\n",
    "            with open(os.path.join(folder,doc)) as f:\n",
    "                if doc[9:18][-1] == '_':\n",
    "                    NextDayReturn = np.float(0)\n",
    "                else:\n",
    "                    NextDayReturn = np.float(doc[9:18])\n",
    "                \n",
    "                articles.append({'BOW':bag_of_words(clean_text(''.join(f))),\n",
    "                                 'Ticker':ticker,\n",
    "                                 'ReleaseDate':doc[:4]+'-'+doc[4:6]+'-'+doc[6:8],\n",
    "                                 'NextDayReturn':NextDayReturn})\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_training=struct_article_data(TrainingDataPath,TrainingTickerList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "articles_training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "articles_training[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgn(v):\n",
    "    if v>0:\n",
    "        return 1;\n",
    "    if v<0:\n",
    "        return 0;\n",
    "    if v==0:\n",
    "        return 0.5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_charged_words(articles, alpha_pos, alpha_neg, kappa, stop_words):\n",
    "    occurance=defaultdict(int)\n",
    "    pos_occr=defaultdict(int)\n",
    "    neg_occr=defaultdict(int)\n",
    "    for article in articles:\n",
    "        if sgn(article['NextDayReturn'])==1:\n",
    "            flag=(1,0)\n",
    "        elif sgn(article['NextDayReturn'])==0:\n",
    "            flag=(0,1)\n",
    "        else:\n",
    "            flag=(0,0)\n",
    "        \n",
    "        for key, value in article['BOW'].items():\n",
    "            if value>0 and key not in stop_words:\n",
    "                occurance[key] += 1\n",
    "                pos_occr[key] += flag[0]\n",
    "                neg_occr[key] += flag[1]\n",
    "    \n",
    "    sentiment_charged_words=[]\n",
    "    pos_words=[]\n",
    "    neg_words=[]\n",
    "    for word, count in occurance.items():\n",
    "        if (count>kappa) and (pos_occr[word]/count > (0.5+alpha_pos)):\n",
    "            pos_words.append(word)\n",
    "            sentiment_charged_words.append(word)\n",
    "        if (count>kappa) and (neg_occr[word]/count > (0.5+alpha_neg)):\n",
    "            neg_words.append(word)\n",
    "            sentiment_charged_words.append(word)\n",
    "        \n",
    "    return [pos_words,neg_words,sentiment_charged_words,occurance,pos_occr,neg_occr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_matrix(articles, sentiment_charged_words):\n",
    "    DocMatrix=np.zeros((len(articles),len(sentiment_charged_words)))\n",
    "    for i in range(len(articles)):\n",
    "        DocVector=[articles[i]['BOW'][word] for word in sentiment_charged_words]\n",
    "        if np.sum(DocVector) != 0:\n",
    "            DocMatrix[i]=DocVector/np.sum(DocVector)\n",
    "        else:\n",
    "            DocMatrix[i]=DocVector\n",
    "    \n",
    "    return DocMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_sentiment_score(articles):\n",
    "    SentimentScoreMatrix=np.zeros((2,len(articles)))\n",
    "    SentimentScoreMatrix[0]=pd.Series([ article['NextDayReturn'] for article in articles ]).rank().values/len(articles)\n",
    "    SentimentScoreMatrix[1]=1-SentimentScoreMatrix[0]\n",
    "    \n",
    "    return SentimentScoreMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimated_words_distribution(DocMatrix, SSMatrix):\n",
    "    #SSMatrix: SentimentScoreMatrix\n",
    "    WordsDistribution=DocMatrix.T.dot(SSMatrix.T).dot(np.linalg.pinv(SSMatrix.dot(SSMatrix.T)))\n",
    "    WordsDistribution=np.where(WordsDistribution<0,0,WordsDistribution)\n",
    "    WordsDistribution=WordsDistribution/WordsDistribution.sum(axis=0)\n",
    "    \n",
    "    return WordsDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_result(articles_training,alpha_pos,alpha_neg,kappa,stop_words):\n",
    "    # get_sentiment_charged_words(articles, alpha_pos, alpha_neg, kappa):\n",
    "    [pos_words,neg_words,sentiment_charged_words,occurance,pos_occr,neg_occr]=\\\n",
    "                               get_sentiment_charged_words(articles_training, alpha_pos, alpha_neg, kappa,stop_words)\n",
    "\n",
    "    # get_document_matrix(articles, sentiment_charged_words):\n",
    "    DocMatrix=get_document_matrix(articles_training, sentiment_charged_words)\n",
    "\n",
    "    # get_training_sentiment_score(articles):\n",
    "    SentimentScoreMatrix=get_training_sentiment_score(articles_training)\n",
    "\n",
    "    # get_estimated_words_distribution(DocMatrix, SSMatrix):\n",
    "    WordsDistribution=get_estimated_words_distribution(DocMatrix, SentimentScoreMatrix)\n",
    "    \n",
    "    return [pos_words,neg_words,sentiment_charged_words,occurance,pos_occr,neg_occr,WordsDistribution]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score_prediction(bow, SCWords, WordsDist, lamb, InitGuess):\n",
    "    #bow: BoW of the new article\n",
    "    #SCWords: sentiment-charged words list\n",
    "    #WordsDist: trained words distribution (of sentiment-charged words), 2darray\n",
    "    #lamb: coefficient of penalty, i.e learning rate\n",
    "    #InitGuess: initial guess of sentiment score\n",
    "    DocVector=[ bow[word] for word in SCWords ]\n",
    "    if np.sum(DocVector) != 0:\n",
    "        DocVector=DocVector/np.sum(DocVector)\n",
    "    else:\n",
    "        DocVector=np.array(DocVector)\n",
    "    \n",
    "    def neg_penalized_likelihood(SentimentScore):\n",
    "        q = SentimentScore*WordsDist.T[0] + (1-SentimentScore)*WordsDist.T[1]\n",
    "        val=np.sum([ DocVector[i]*np.log(q[i]) for i in range(DocVector.shape[0]) ]) \\\n",
    "                  + lamb*np.log(SentimentScore*(1-SentimentScore))\n",
    "        return -val\n",
    "    \n",
    "    result=sp.optimize.minimize(neg_penalized_likelihood,InitGuess,method='SLSQP',bounds=[(0,1)])\n",
    "    \n",
    "    return result.x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now grid search for appopriate threshold  ( $\\alpha_+$, $\\alpha_-$, $\\kappa$, $\\lambda$ )\n",
    "#### Analysis is based on single factor analysis, mainly focusing on IC / IR ( ?? )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score_prediction_table(sentiment_charged_words,WordsDistribution,articles_validation,lamb):\n",
    "    \n",
    "    ###  Now we have word distribution from articles_training ###\n",
    "    ###  Tend to get sentiment score prediction of every article in articles_validation ###\n",
    "    \n",
    "    ArticlePrediction={} # save prediction of article which is in articles_validation, form is like:\n",
    "    \n",
    "    #   {'AAPL': {'2019-01-16': DefaultDict_float{'PredictionScore':2.35252,\n",
    "    #                                             'count': 3\n",
    "    #                                            }\n",
    "    #             '2019-01-17': DefaultDict_float{'PredictionScore':6.25253,\n",
    "    #                                             'count': 6\n",
    "    #                                            }\n",
    "    #            }\n",
    "    #\n",
    "    #    'MSFT': {'2019-01-16': DefaultDict_float{'PredictionScore':2.35252,\n",
    "    #                                             'count': 3\n",
    "    #                                            }\n",
    "    #             '2019-01-17': DefaultDict_float{'PredictionScore':6.25253,\n",
    "    #                                             'count': 6\n",
    "    #                                            }\n",
    "    #            }\n",
    "    #\n",
    "    #    }\n",
    "    \n",
    "    TrackingNum=0\n",
    "    print('Prediction work start. Total workload will be: ',len(articles_validation))\n",
    "    \n",
    "    for article in articles_validation:\n",
    "        # get_sentiment_score_prediction(bow, SCWords, WordsDist, lamb, InitGuess):\n",
    "        PredictedScore=get_sentiment_score_prediction(article['BOW'],sentiment_charged_words,WordsDistribution,lamb,0.5)\n",
    "        \n",
    "        TrackingNum += 1\n",
    "        print('Prediction for a new article done: ',article['Ticker'],', ',article['ReleaseDate'],', TrackingNum: ',TrackingNum)\n",
    "        \n",
    "        if article['Ticker'] not in ArticlePrediction.keys():\n",
    "            ArticlePrediction[article['Ticker']]={}\n",
    "        \n",
    "        if article['ReleaseDate'] not in ArticlePrediction[article['Ticker']].keys():\n",
    "            ArticlePrediction[article['Ticker']][article['ReleaseDate']]=defaultdict(float)\n",
    "        \n",
    "        ArticlePrediction[article['Ticker']][article['ReleaseDate']]['Prediction'] += PredictedScore\n",
    "        ArticlePrediction[article['Ticker']][article['ReleaseDate']]['count'] += 1\n",
    "    \n",
    "    \n",
    "    print('Prediction all complete')\n",
    "    \n",
    "    date=set()\n",
    "    for score in ArticlePrediction.values():\n",
    "        date |= set(score.keys())\n",
    "    \n",
    "    \n",
    "    print('Start creating table')\n",
    "    \n",
    "    table=pd.DataFrame(columns=ArticlePrediction.keys(), index=date)\n",
    "    for ticker in ArticlePrediction.keys():\n",
    "        for date_ in date:\n",
    "            if date_ in ArticlePrediction[ticker].keys():\n",
    "                PredictionScore=ArticlePrediction[ticker][date_]['Prediction']\n",
    "                count=ArticlePrediction[ticker][date_]['count']\n",
    "                table.loc[date_,ticker]=PredictionScore/count\n",
    "            else:\n",
    "                table.loc[date_,ticker]=np.nan\n",
    "    \n",
    "    print('Create table complete')\n",
    "    \n",
    "    table.index=pd.to_datetime(table.index,format='%Y-%m-%d')\n",
    "    table=table.sort_index(axis=0,ascending=True)\n",
    "    \n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestDataPath=os.path.abspath('./Data/test')\n",
    "TestTickerList=os.listdir(TestDataPath)\n",
    "if '.DS_Store' in TestTickerList:\n",
    "    TestTickerList.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestTickerList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_test=struct_article_data(TestDataPath,TestTickerList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "articles_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(articles_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[pos_words,neg_words,sentiment_charged_words,occurance,pos_occr,neg_occr,WordsDistribution]=\\\n",
    "                                                       get_training_result(articles_training,0.13,0.2,20,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12),dpi=300)\n",
    "PosWordCloud = WordCloud(background_color=\"white\",width=5000, height=3000, margin=2).generate(' '.join(pos_words))\n",
    "plt.imshow(PosWordCloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12),dpi=300)\n",
    "NegWordCloud = WordCloud(background_color=\"white\",width=5000, height=3000, margin=2).generate(' '.join(neg_words))\n",
    "plt.imshow(NegWordCloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment_charged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentiment_charged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_occr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "WordsDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(WordsDistribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentimentTableTrain=get_sentiment_score_prediction_table(sentiment_charged_words,WordsDistribution,articles_training,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SentimentTableTest=get_sentiment_score_prediction_table(sentiment_charged_words,WordsDistribution,articles_test,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SentimentTableTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SentimentTableTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_test=pd.read_csv('./price_test.csv',index_col=0)\n",
    "price_test.index=pd.to_datetime(price_test.index,format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_train=pd.read_csv('./price_train.csv',index_col=0)\n",
    "price_train.index=pd.to_datetime(price_train.index,format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "price_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return_validation=price_validation.pct_change().shift(-1).iloc[:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.any(np.isnan(return_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_train=al.utils.get_clean_factor_and_forward_returns(SentimentTableTrain.stack().dropna(),\n",
    "                                                          price_train,\n",
    "                                                          by_group=False,\n",
    "                                                          quantiles=5,\n",
    "                                                          periods=(1,2,3),\n",
    "                                                          filter_zscore=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "al.tears.create_returns_tear_sheet(factor_train, group_neutral=False, by_group=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "al.tears.create_information_tear_sheet(factor_train, by_group=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al.tears.create_turnover_tear_sheet(factor_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_test=al.utils.get_clean_factor_and_forward_returns(SentimentTableTest.stack().dropna(),\n",
    "                                                          price_test,\n",
    "                                                          by_group=False,\n",
    "                                                          quantiles=5,\n",
    "                                                          periods=(1,2,3),\n",
    "                                                          filter_zscore=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### factor value 从小到大排, 小的落入小号组, 大的落到大号组, 小号组是bottom, 大号组是top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "al.tears.create_returns_tear_sheet(factor_test, group_neutral=False, by_group=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "al.tears.create_information_tear_sheet(factor_test, by_group=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "al.tears.create_turnover_tear_sheet(factor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
