{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#get url from the home page\n",
    "#get all the news' url on n pages\n",
    "n = 100\n",
    "url_list = []\n",
    "for i in range(1, n+1):\n",
    "    #get page i\n",
    "    #businesshome = 'https://www.reuters.com/news/archive/businessnews?view=page&page={}&pageSize=10'.format(i)\n",
    "    techhome = 'https://www.reuters.com/news/archive/technologynews?view=page&page={}&pageSize=10'.format(i)\n",
    "    #newspage = 'https://www.reuters.com/news/archive/businessnews?view=page&page={}&pageSize=10'.format(i)\n",
    "    html0 = urllib.request.urlopen(techhome).read()\n",
    "    soup0 = BeautifulSoup(html0)\n",
    "    #print(soup0.prettify())\n",
    "    \n",
    "    #get all the url on page i\n",
    "    l = soup0.find_all('div', attrs={\"class\":\"story-content\"})\n",
    "    for li in l:\n",
    "        #print(li.a['href'])\n",
    "        url_list.append('https://www.reuters.com'+li.a['href'])\n",
    "\n",
    "#url_list = list(set(url_list))   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "rows = []\n",
    "with open('sp500.csv') as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    \n",
    "    for row in spamreader:\n",
    "        rows.append(row)\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pair bloomberg ticker with ruters instrument code\n",
    "\n",
    "n=0\n",
    "\n",
    "for row in rows:\n",
    "    #print(row[0])\n",
    "    htmla = urllib.request.urlopen('https://www.reuters.com/companies/'+row[0]+'.N').read()\n",
    "    #print('https://www.reuters.com/companies/'+row[0]+'.N')\n",
    "    soupa = BeautifulSoup(htmla)\n",
    "    url_lista = []\n",
    "    lines = soupa.find_all('a', attrs={\"class\":\"TextLabel__text-label___3oCVw TextLabel__black-to-orange___23uc0 TextLabel__medium___t9PWg MarketStoryItem-headline-2cgfz\"})\n",
    "    for line in lines:\n",
    "        url_lista.append(line['href'])\n",
    "    #print(url_lista)\n",
    "    if url_lista != []:\n",
    "        row[0] += '.N'\n",
    "        n+=1\n",
    "        print(n, row[0])\n",
    "    \n",
    "    else:\n",
    "        htmlb = urllib.request.urlopen('https://www.reuters.com/companies/'+row[0]+'.O').read()\n",
    "        #print('https://www.reuters.com/companies/'+row[0]+'.O')\n",
    "        soupb = BeautifulSoup(htmlb)\n",
    "        url_listb = []\n",
    "        lines = soupb.find_all('a', attrs={\"class\":\"TextLabel__text-label___3oCVw TextLabel__black-to-orange___23uc0 TextLabel__medium___t9PWg MarketStoryItem-headline-2cgfz\"})\n",
    "        for line in lines:\n",
    "            url_listb.append(line['href'])\n",
    "        \n",
    "        if url_listb != []:\n",
    "            row[0] += '.O'\n",
    "            n+=1\n",
    "            print(n, row[0])\n",
    "        else:\n",
    "            n+=1\n",
    "            print(n, row[0], 'Wrong')\n",
    "    \n",
    "    \n",
    "    #soup5 = BeautifulSoup(html5)\n",
    "    #print(soup5.prettify())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('sp500.csv') as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    \n",
    "    for d in spamreader:\n",
    "        data.append(d)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_list = []\n",
    "for a, b in zip(data, rows):\n",
    "    ticker_list.append([a[0], b[0]])\n",
    "    \n",
    "ticker_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "driver=webdriver.Chrome(executable_path='/Users/xiaobai/Downloads/chromedriver')\n",
    "\n",
    "#stop every time after scroll down\n",
    "def execute_times(times):\n",
    "    for i in range(times):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "\n",
    "for i in range(72, 73):\n",
    "    \n",
    "    #create a folder for ticker i\n",
    "    path = '/Users/xiaobai/Desktop/New_data/{}'.format(ticker_list[i][0])\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "\n",
    "    #get the company's page and roll down\n",
    "    roll = 0\n",
    "    driver.get(\"https://www.reuters.com/companies/\"+ticker_list[i][1])\n",
    "    html_ = driver.page_source\n",
    "    soup_ = BeautifulSoup(html_)\n",
    "    #ago = soup_.find_all('time', attrs={\"class\":\"TextLabel__text-label___3oCVw TextLabel__gray___1V4fk TextLabel__regular___2X0ym MarketStoryItem-date-H-tta\"})\n",
    "    execute_times(200)\n",
    "    time.sleep(1)\n",
    "#     while ago[-1].text != '6 years ago' and roll<13:\n",
    "#         execute_times(10)\n",
    "#         roll += 1\n",
    "#         html_ = driver.page_source\n",
    "#         soup_ = BeautifulSoup(html_)\n",
    "#         ago = soup_.find_all('time', attrs={\"class\":\"TextLabel__text-label___3oCVw TextLabel__gray___1V4fk TextLabel__regular___2X0ym MarketStoryItem-date-H-tta\"})\n",
    "\n",
    "    #get all the url on the page\n",
    "    url_list = []\n",
    "    html_ = driver.page_source\n",
    "    soup_ = BeautifulSoup(html_)\n",
    "    lines = soup_.find_all('a', attrs={\"class\":\"TextLabel__text-label___3oCVw TextLabel__black-to-orange___23uc0 TextLabel__medium___t9PWg MarketStoryItem-headline-2cgfz\"})\n",
    "    for line in lines:\n",
    "        url_list.append(line['href'])\n",
    "        \n",
    "    #get text, date in each url\n",
    "    count = 0\n",
    "    for url in url_list:\n",
    "        try:\n",
    "            html = urllib.request.urlopen(url).read()\n",
    "            soup = BeautifulSoup(html)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        #get ticker(s)\n",
    "#         lines = soup.find_all('span')\n",
    "#         tickers = []\n",
    "#         for line in lines:\n",
    "#             try:\n",
    "#                 symbol = line['id']\n",
    "#                 symbol = symbol.split('_')\n",
    "#                 symbol = symbol[1]\n",
    "#                 tickers.append(symbol)\n",
    "#             except:\n",
    "#                 pass\n",
    "        #if len(tickers)>1: continue\n",
    "            \n",
    "        #get date\n",
    "        try:\n",
    "            dateline = soup.find_all('div', attrs={\"class\":\"ArticleHeader_date\"})\n",
    "            date = dateline[0].text.split('/')[0][:-1]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # get text\n",
    "        links=soup.find_all('p')\n",
    "        article=[]\n",
    "        for link in links:\n",
    "            article.append(link.text)\n",
    "            text = ''.join(article)\n",
    "        \n",
    "        # ouput news to each file\n",
    "        outfile=open(path+'/'+str(count)+'_'+date+'.txt','w')\n",
    "        outfile.write(date)\n",
    "        outfile.write('\\n')\n",
    "        outfile.write(text)\n",
    "        outfile.write('\\n')\n",
    "        outfile.close()\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
